# èœœèœ‚æ£€æµ‹Streamlitä»£ç 
import os
import cv2
import numpy as np
import warnings
import paddle
import paddle.nn as nn
import paddle.nn.functional as F
import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
import io


plt.switch_backend('Agg')

# -------------------------- 1. å…¨å±€é…ç½®ç±» --------------------------
class Config:
    CONF_THRESHOLD = 0.3
    DATA_ROOT = "/home/aistudio/bee_dataset"
    MODEL_PATH = "/home/aistudio/models/best_model.pdparams"
    NUM_ANCHORS = 5
    TARGET_SIZE = (224, 224)
    IMAGE_EXTENSIONS = ('.png', '.jpg', '.jpeg', '.bmp')

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="paddle.nn.layer.norm")

# -------------------------- 2. å·¥å…·å‡½æ•° --------------------------
def draw_boxes(img, boxes, color=(0, 255, 0), text_color=(255, 255, 255)):
    img_copy = img.copy()
    h, w = img_copy.shape[:2]
    for box in boxes:
        if len(box) == 5:
            x1, y1, x2, y2, conf = box
            label = f"{conf:.2f}"
        else:
            x1, y1, x2, y2 = box
            label = ""
        
        x1, y1, x2, y2 = int(round(x1)), int(round(y1)), int(round(x2)), int(round(y2))
        x1 = max(0, min(x1, w-1))
        y1 = max(0, min(y1, h-1))
        x2 = max(0, min(x2, w-1))
        y2 = max(0, min(y2, h-1))
        
        cv2.rectangle(img_copy, (x1, y1), (x2, y2), color, 2)
        if label:
            (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            text_x, text_y = x1, y1 - 10 if y1 > 10 else y1 + text_h + 5
            cv2.rectangle(img_copy, (text_x, text_y - text_h), (text_x + text_w, text_y), color, -1)
            cv2.putText(img_copy, label, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)
    return img_copy

# -------------------------- 3. æ•°æ®é›†ç±» --------------------------
class BeeDataset:
    def __init__(self, root_dir, split):
        self.root_dir = root_dir
        self.split = split
        self.images_dir = os.path.join(root_dir, split)
        self.ann_file = os.path.join(self.images_dir, "_annotations.coco.json")
        self.image_files = self._get_valid_images()
        # åŠ è½½æ ‡æ³¨å¹¶æ”¶é›†çŠ¶æ€ä¿¡æ¯è€Œéç›´æ¥æ˜¾ç¤º
        self.annotations, self.load_status = self._load_coco_annotations()

    def _get_valid_images(self):
        if not os.path.exists(self.images_dir):
            # è¿”å›çŠ¶æ€ä¿¡æ¯è€Œéç›´æ¥è°ƒç”¨st.warning
            return []
        return [f for f in os.listdir(self.images_dir) if f.lower().endswith(Config.IMAGE_EXTENSIONS)]

    def _load_coco_annotations(self):
        ann_map = {}
        status = {"success": True, "message": ""}
        if not os.path.exists(self.ann_file):
            status = {
                "success": False, 
                "message": f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.ann_file}ï¼ˆå°†ä½¿ç”¨ç©ºæ ‡æ³¨ï¼‰"
            }
            return ann_map, status
        
        try:
            import json
            with open(self.ann_file, 'r', encoding='utf-8') as f:
                coco = json.load(f)
            
            img_id_to_name = {img['id']: img['file_name'] for img in coco['images']}
            img_id_to_boxes = {}
            for ann in coco['annotations']:
                if ann.get('category_id', 0) != 1:
                    continue
                img_id = ann['image_id']
                x1, y1, w, h = ann['bbox']
                x2, y2 = x1 + w, y1 + h
                img_id_to_boxes.setdefault(img_id, []).append([x1, y1, x2, y2])
            
            for img_id, boxes in img_id_to_boxes.items():
                fname = img_id_to_name.get(img_id)
                if not fname or fname not in self.image_files:
                    continue
                img_path = os.path.join(self.images_dir, fname)
                img = cv2.imread(img_path)
                if img is None:
                    continue
                h, w = img.shape[:2]
                norm_boxes = [[x1/w, y1/h, x2/w, y2/h] for x1,y1,x2,y2 in boxes]
                ann_map[fname] = norm_boxes
            
            status["message"] = f"{self.split}é›†ï¼šåŠ è½½{len(ann_map)}å¼ å¸¦æ ‡æ³¨å›¾åƒ"
            return ann_map, status
        except Exception as e:
            status = {
                "success": False, 
                "message": f"åŠ è½½æ ‡æ³¨å¤±è´¥ï¼š{str(e)}ï¼ˆå°†ä½¿ç”¨ç©ºæ ‡æ³¨ï¼‰"
            }
            return ann_map, status

    def get_image(self, fname):
        img_path = os.path.join(self.images_dir, fname)
        img = cv2.imread(img_path)
        if img is None:
            
            return np.ones((*Config.TARGET_SIZE, 3), dtype=np.uint8) * 240, [], f"å›¾åƒåŠ è½½å¤±è´¥ï¼š{fname}"
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img_rgb.shape[:2]
        norm_boxes = self.annotations.get(fname, [])
        real_boxes = [[x1*w, y1*h, x2*w, y2*h] for x1,y1,x2,y2 in norm_boxes]
        return img_rgb, real_boxes, ""

# -------------------------- 4. æ£€æµ‹æ¨¡å‹ç±» --------------------------
class ConvBlock(nn.Layer):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2D(in_channels, out_channels, 3, 1, 1)
        self.bn = nn.BatchNorm2D(out_channels)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2D(2, 2)

    def forward(self, x):
        return self.pool(self.relu(self.bn(self.conv(x))))

class FeatureExtractor(nn.Layer):
    def __init__(self):
        super().__init__()
        self.block1 = ConvBlock(3, 16)
        self.block2 = ConvBlock(16, 32)
        self.block3 = ConvBlock(32, 64)
        self.block4 = ConvBlock(64, 128)
        self.block5 = ConvBlock(128, 256)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.block5(x)
        return x

class BboxPredictor(nn.Layer):
    def __init__(self, in_features, num_anchors=5):
        super().__init__()
        self.fc1 = nn.Linear(in_features, 1024)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(1024, 7*7*num_anchors*5)

    def forward(self, x):
        batch_size = x.shape[0]
        x_flatten = paddle.flatten(x, 1)
        x = F.relu(self.fc1(x_flatten))
        x = self.dropout1(x)
        x = self.fc2(x)
        return paddle.reshape(x, [batch_size, 7, 7, 5, 5])

class BeeDetectionModel(nn.Layer):
    def __init__(self, num_anchors=5):
        super().__init__()
        self.feature_extractor = FeatureExtractor()
        with paddle.no_grad():
            test_input = paddle.randn([1, 3, *Config.TARGET_SIZE], dtype='float32')
            test_feature = self.feature_extractor(test_input)
            in_features = test_feature.numel().item()
        self.bbox_predictor = BboxPredictor(in_features, num_anchors)
        self.anchors = self._init_anchors(num_anchors)

    def _init_anchors(self, num_anchors):
        anchors_init = [[0.2,0.2], [0.4,0.4], [0.6,0.6], [0.8,0.8], [1.0,1.0]]
        anchor_status = {"warning": ""}
        if num_anchors != len(anchors_init):
            anchor_status["warning"] = f"é”šæ¡†æ•°é‡ä¸åŒ¹é…ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º{len(anchors_init)}"
            anchors_init = anchors_init[:num_anchors]
        anchors_tensor = paddle.to_tensor(anchors_init, dtype='float32')
        anchors_param = self.create_parameter(
            shape=anchors_tensor.shape,
            dtype=anchors_tensor.dtype,
            default_initializer=nn.initializer.Assign(anchors_tensor)
        )
        self.add_parameter('anchors_param', anchors_param)
        return anchors_param, anchor_status

    def forward(self, x):
        features = self.feature_extractor(x)
        pred = self.bbox_predictor(features)
        return pred[..., :4], F.sigmoid(pred[..., 4:]), self.anchors

# -------------------------- 5. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° --------------------------
def predict_image(model, input_img, conf_threshold):
    img_resized = cv2.resize(input_img, Config.TARGET_SIZE)
    img_norm = img_resized / 255.0
    img_chw = np.transpose(img_norm, (2, 0, 1))
    input_tensor = paddle.to_tensor(img_chw[np.newaxis, ...], dtype='float32')

    with paddle.no_grad():
        bbox_pred, conf_pred, anchors = model(input_tensor)

    pred_boxes = []
    bbox_np = bbox_pred.numpy()[0]
    conf_np = conf_pred.numpy()[0]
    anchors_np = anchors[0].numpy()  
    img_h, img_w = input_img.shape[:2]

    for grid_y in range(7):
        for grid_x in range(7):
            for anchor_idx in range(anchors_np.shape[0]):
                conf = conf_np[grid_y, grid_x, anchor_idx, 0]
                if conf < conf_threshold:
                    continue
                
                tx, ty, tw, th = bbox_np[grid_y, grid_x, anchor_idx]
                cx = (grid_x + tx) / 7
                cy = (grid_y + ty) / 7
                anchor_w, anchor_h = anchors_np[anchor_idx]
                w = anchor_w * np.exp(tw)
                h = anchor_h * np.exp(th)
                
                x1 = (cx - w/2) * img_w
                y1 = (cy - h/2) * img_h
                x2 = (cx + w/2) * img_w
                y2 = (cy + h/2) * img_h
                pred_boxes.append((x1, y1, x2, y2, conf))
    
    return pred_boxes

def get_dataset_stats(train_dataset, valid_dataset, test_dataset):
    def count_bees(dataset):
        total = 0
        sample_num = min(50, len(dataset.image_files))
        for fname in dataset.image_files[:sample_num]:
            total += len(dataset.annotations.get(fname, []))
        return total * len(dataset.image_files) // sample_num if sample_num > 0 else 0

    train_img = len(train_dataset.image_files)
    valid_img = len(valid_dataset.image_files)
    test_img = len(test_dataset.image_files)
    total_img = train_img + valid_img + test_img

    train_bee = count_bees(train_dataset)
    valid_bee = count_bees(valid_dataset)
    test_bee = count_bees(test_dataset)
    total_bee = train_bee + valid_bee + test_bee

    return {
        "split": ["è®­ç»ƒé›†", "éªŒè¯é›†", "æµ‹è¯•é›†", "æ€»è®¡"],
        "image_count": [train_img, valid_img, test_img, total_img],
        "bee_count": [train_bee, valid_bee, test_bee, total_bee],
        "avg_bees_per_image": [
            round(train_bee/train_img, 2) if train_img>0 else 0,
            round(valid_bee/valid_img, 2) if valid_img>0 else 0,
            round(test_bee/test_img, 2) if test_img>0 else 0,
            round(total_bee/total_img, 2) if total_img>0 else 0
        ]
    }

# -------------------------- 6. èµ„æºåˆå§‹åŒ– --------------------------
@st.cache(allow_output_mutation=True)
def init_resources():
    """ä»…åˆå§‹åŒ–èµ„æºï¼Œè¿”å›æ‰€æœ‰éœ€è¦çš„çŠ¶æ€ä¿¡æ¯ä½†ä¸ç›´æ¥è¿›è¡ŒUIäº¤äº’"""
    try:
        # åŠ è½½æ•°æ®é›†å¹¶æ”¶é›†çŠ¶æ€ä¿¡æ¯
        train_dataset = BeeDataset(Config.DATA_ROOT, "train")
        valid_dataset = BeeDataset(Config.DATA_ROOT, "valid")
        test_dataset = BeeDataset(Config.DATA_ROOT, "test")
        
        # æ”¶é›†æ•°æ®é›†åŠ è½½çŠ¶æ€
        dataset_statuses = [
            train_dataset.load_status,
            valid_dataset.load_status,
            test_dataset.load_status
        ]

        # åˆå§‹åŒ–æ¨¡å‹
        model = BeeDetectionModel(Config.NUM_ANCHORS)
        model_loaded = False
        model_msg = ""
        if os.path.exists(Config.MODEL_PATH):
            try:
                model.set_state_dict(paddle.load(Config.MODEL_PATH))
                model_loaded = True
                model_msg = f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š{Config.MODEL_PATH}"
            except Exception as e:
                model_msg = f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡"
        else:
            model_msg = f"æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆè·¯å¾„ï¼š{Config.MODEL_PATH}ï¼‰"
        model.eval()

        # è·å–é”šæ¡†çŠ¶æ€ä¿¡æ¯
        anchor_status = model.anchors[1]  # ä»æ¨¡å‹è·å–é”šæ¡†çŠ¶æ€

        # è®¡ç®—æ•°æ®é›†ç»Ÿè®¡
        dataset_stats = get_dataset_stats(train_dataset, valid_dataset, test_dataset)

        # è¿”å›æ‰€æœ‰å¿…è¦ä¿¡æ¯å’ŒçŠ¶æ€
        return {
            "datasets": (train_dataset, valid_dataset, test_dataset),
            "model": model,
            "dataset_stats": dataset_stats,
            "status": "success",
            "messages": {
                "dataset": dataset_statuses,
                "model": model_msg,
                "anchor": anchor_status["warning"]
            }
        }

    except Exception as e:
        return {
            "datasets": None,
            "model": None,
            "dataset_stats": None,
            "status": "error",
            "messages": {"error": str(e)}
        }

# -------------------------- 7. ä¸»ç•Œé¢ï¼ˆæ‰€æœ‰UIäº¤äº’åœ¨æ­¤å¤„å¤„ç†ï¼‰ --------------------------
def main():
    st.set_page_config(
        page_title="ğŸ èœœèœ‚æ£€æµ‹å¯è§†åŒ–å¹³å°",
        page_icon="ğŸ",
        layout="wide",
        initial_sidebar_state="collapsed"
    )
    st.title("ğŸ èœœèœ‚æ£€æµ‹å¯è§†åŒ–å¹³å°")
    st.markdown("---")

    # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
    init_status = st.empty()
    init_status.info("æ­£åœ¨åˆå§‹åŒ–èµ„æºï¼Œè¯·ç¨å€™...")

    # è°ƒç”¨ç¼“å­˜å‡½æ•°
    resources = init_resources()

    # å¤„ç†åˆå§‹åŒ–ç»“æœå¹¶æ˜¾ç¤ºUIåé¦ˆ
    if resources["status"] == "success":
        train_dataset, valid_dataset, test_dataset = resources["datasets"]
        model = resources["model"]
        dataset_stats = resources["dataset_stats"]
        
        # æ˜¾ç¤ºæ•°æ®é›†åŠ è½½çŠ¶æ€
        for status in resources["messages"]["dataset"]:
            if status["success"]:
                st.success(status["message"])
            else:
                st.warning(status["message"])
        
        # æ˜¾ç¤ºæ¨¡å‹åŠ è½½çŠ¶æ€
        if "æˆåŠŸåŠ è½½" in resources["messages"]["model"]:
            st.success(resources["messages"]["model"])
        else:
            st.warning(resources["messages"]["model"])
        
        # æ˜¾ç¤ºé”šæ¡†çŠ¶æ€
        if resources["messages"]["anchor"]:
            st.warning(resources["messages"]["anchor"])
        
        init_status.success("ğŸ‰ æ‰€æœ‰èµ„æºåˆå§‹åŒ–å®Œæˆï¼")
    else:
        init_status.error(f"âŒ èµ„æºåˆå§‹åŒ–å¤±è´¥ï¼š{resources['messages']['error']}")
        
        class DummyDataset:
            def __init__(self):
                self.image_files = []
                self.annotations = {}
                self.load_status = {"success": False, "message": "ä½¿ç”¨é»˜è®¤ç©ºæ•°æ®é›†"}
            def get_image(self, fname):
                return np.ones((*Config.TARGET_SIZE, 3), dtype=np.uint8)*240, [], ""
        dummy_ds = DummyDataset()
        train_dataset = valid_dataset = test_dataset = dummy_ds
        model = BeeDetectionModel()
        model.eval()
        dataset_stats = {
            "split": ["è®­ç»ƒé›†", "éªŒè¯é›†", "æµ‹è¯•é›†", "æ€»è®¡"],
            "image_count": [0, 0, 0, 0],
            "bee_count": [0, 0, 0, 0],
            "avg_bees_per_image": [0.0, 0.0, 0.0, 0.0]
        }

    # æ ‡ç­¾é¡µå¯¼èˆª
    tab1, tab2, tab3 = st.tabs(["1. æ•°æ®é›†ç»Ÿè®¡", "2. å›¾åƒæµè§ˆ", "3. å®æ—¶æ£€æµ‹"])

    with tab1:
       st.subheader("ğŸ“Š Dataset Distribution Overview")
       st.markdown("### Basic Statistics Table")
       stats_df = pd.DataFrame(dataset_stats)
       st.dataframe(stats_df, use_container_width=True)

       st.markdown("### Number of Images in Each Dataset")
       fig1, ax1 = plt.subplots(figsize=(8, 4))
       splits = dataset_stats["split"][:-1]
       img_counts = dataset_stats["image_count"][:-1]
       colors1 = ["#FF6B6B", "#4ECDC4", "#45B7D1"]
       bars1 = ax1.bar(splits, img_counts, color=colors1, alpha=0.8, edgecolor="white", linewidth=2)
      
       for bar, count in zip(bars1, img_counts):
           height = bar.get_height()
           ax1.text(
               bar.get_x() + bar.get_width()/2., height + max(img_counts)*0.01,
               str(count), ha="center", va="bottom", fontsize=12, fontweight="bold"
           )
      
       ax1.set_ylabel("Number of Images", fontsize=12, fontweight="bold")
       ax1.set_title("Image Distribution in Train/Validation/Test Sets", fontsize=14, fontweight="bold", pad=20)
       ax1.grid(axis="y", alpha=0.3, linestyle="--")
       plt.tight_layout()
       st.pyplot(fig1)

       st.markdown("### Proportion of Bee Counts by Dataset")
       fig2, ax2 = plt.subplots(figsize=(6, 6))
       bee_counts = dataset_stats["bee_count"][:-1]
       colors2 = ["#FF6B6B", "#4ECDC4", "#45B7D1"]
       wedges, texts, autotexts = ax2.pie(
           bee_counts, labels=splits, colors=colors2, autopct="%1.1f%%",
           startangle=90, textprops={"fontsize": 12}, wedgeprops={"edgecolor": "white", "linewidth": 2}
       )
      
       for autotext in autotexts:
           autotext.set_color("white")
           autotext.set_fontweight("bold")
      
       ax2.set_title("Proportion of Bee Counts by Dataset", fontsize=14, fontweight="bold", pad=20)
       plt.tight_layout()
       st.pyplot(fig2)

    with tab2:
        st.subheader("ğŸ” èœœèœ‚å›¾åƒæµè§ˆï¼ˆç»¿è‰²æ¡†ä¸ºçœŸå®æ ‡æ³¨ï¼‰")
        
        st.markdown("#### ç­›é€‰æ¡ä»¶")
        
        split_col, keyword_col = st.columns(2)
        with split_col:
            selected_split = st.selectbox(
                "é€‰æ‹©æ•°æ®é›†",
                options=["è®­ç»ƒé›†", "éªŒè¯é›†", "æµ‹è¯•é›†"],
                index=0,
                help="é€‰æ‹©è¦æµè§ˆçš„æ•°æ®é›†åˆ’åˆ†"
            )
        with keyword_col:
            search_keyword = st.text_input(
                "æ–‡ä»¶åå…³é”®è¯",
                placeholder="ä¾‹å¦‚ï¼šbee_001ã€2024",
                help="è¾“å…¥æ–‡ä»¶ååŒ…å«çš„å…³é”®è¯è¿›è¡Œæ£€ç´¢"
            )
        
        
        slider_col, btn_col = st.columns([2, 1])
        with slider_col:
            max_display = st.slider(
                "æœ€å¤šæ˜¾ç¤ºæ•°é‡",
                min_value=3,
                max_value=30,
                value=12,
                step=3,
                help="æ§åˆ¶æ˜¾ç¤ºçš„å›¾åƒæ•°é‡"
            )
        with btn_col:
            if st.button("ğŸ”„ åˆ·æ–°å›¾åƒ"):
                st.experimental_rerun()

        # åˆ†å‰²çº¿åˆ†éš”ç­›é€‰åŒºå’Œå›¾åƒåŒº
        st.markdown("---")
        st.markdown("#### å›¾åƒå±•ç¤º")

        
        if selected_split == "è®­ç»ƒé›†":
            current_ds = train_dataset
        elif selected_split == "éªŒè¯é›†":
            current_ds = valid_dataset
        else:
            current_ds = test_dataset

        # ç­›é€‰åŒ¹é…çš„å›¾åƒ
        matched_files = []
        for fname in current_ds.image_files:
            if search_keyword.lower() in fname.lower():
                matched_files.append(fname)
            if len(matched_files) >= max_display:
                break

        # å¤„ç†æ— å›¾åƒ/æ— åŒ¹é…çš„æƒ…å†µ
        if not current_ds.image_files:
            st.warning("âš ï¸ å½“å‰æ•°æ®é›†æ— å›¾åƒæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„ï¼")
            placeholder = np.ones((300, 400, 3), dtype=np.uint8)*240
            st.image(placeholder, caption="æ— å›¾åƒæ•°æ®", width=400)
        elif not matched_files:
            st.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{search_keyword}ã€çš„å›¾åƒ")
            placeholder = np.ones((300, 400, 3), dtype=np.uint8)*240
            st.image(placeholder, caption="æ— åŒ¹é…å›¾åƒ", width=400)
        else:
            st.success(f"âœ… æ‰¾åˆ° {len(matched_files)} å¼ åŒ¹é…å›¾åƒï¼ˆå…±{len(current_ds.image_files)}å¼ ï¼‰")
            
            
            for i in range(0, len(matched_files), 3):
                
                row_imgs = matched_files[i:i+3]
                img_col1, img_col2, img_col3 = st.columns(3)
                
                # ç¬¬ä¸€å¼ å›¾
                with img_col1:
                    fname = row_imgs[0]
                    img_rgb, real_boxes, error_msg = current_ds.get_image(fname)
                    if error_msg:
                        st.error(error_msg)
                    img_with_boxes = draw_boxes(img_rgb, real_boxes, color=(0, 255, 0))
                    st.image(
                        img_with_boxes,
                        caption=f"{os.path.basename(fname)}\nèœœèœ‚æ•°é‡ï¼š{len(real_boxes)}",
                        use_column_width=True,
                        clamp=True
                    )
                
                # ç¬¬äºŒå¼ å›¾
                if len(row_imgs) >= 2:
                    with img_col2:
                        fname = row_imgs[1]
                        img_rgb, real_boxes, error_msg = current_ds.get_image(fname)
                        if error_msg:
                            st.error(error_msg)
                        img_with_boxes = draw_boxes(img_rgb, real_boxes, color=(0, 255, 0))
                        st.image(
                            img_with_boxes,
                            caption=f"{os.path.basename(fname)}\nèœœèœ‚æ•°é‡ï¼š{len(real_boxes)}",
                            use_column_width=True,
                            clamp=True
                        )
                
                # ç¬¬ä¸‰å¼ å›¾
                if len(row_imgs) >= 3:
                    with img_col3:
                        fname = row_imgs[2]
                        img_rgb, real_boxes, error_msg = current_ds.get_image(fname)
                        if error_msg:
                            st.error(error_msg)
                        img_with_boxes = draw_boxes(img_rgb, real_boxes, color=(0, 255, 0))
                        st.image(
                            img_with_boxes,
                            caption=f"{os.path.basename(fname)}\nèœœèœ‚æ•°é‡ï¼š{len(real_boxes)}",
                            use_column_width=True,
                            clamp=True
                        )
                
                # æ¯è¡Œç»“æŸååŠ ç©ºè¡Œåˆ†éš”
                st.markdown("")

    with tab3:
        st.subheader("ğŸ“¸ èœœèœ‚å®æ—¶æ£€æµ‹ï¼ˆçº¢è‰²æ¡†ä¸ºé¢„æµ‹ç»“æœï¼‰")
        
        st.markdown("#### æ£€æµ‹è®¾ç½®")
        upload_col, threshold_col = st.columns([2, 1])  
        with upload_col:
            uploaded_file = st.file_uploader(
                "é€‰æ‹©å›¾åƒæ–‡ä»¶",
                type=["png", "jpg", "jpeg", "bmp"],
                help="ä¸Šä¼ åŒ…å«èœœèœ‚çš„å›¾åƒæ–‡ä»¶ï¼ˆæ¨èå°ºå¯¸ï¼š500-1000pxï¼‰"
            )
        with threshold_col:
            conf_threshold = st.slider(
                "æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼",
                min_value=0.1,
                max_value=0.9,
                value=Config.CONF_THRESHOLD,
                step=0.05,
                help="å€¼è¶Šé«˜ï¼Œæ£€æµ‹ç»“æœè¶Šä¸¥æ ¼ï¼ˆå‡å°‘è¯¯æ£€ï¼‰ï¼›å€¼è¶Šä½ï¼Œæ£€æµ‹è¶Šçµæ•ï¼ˆå¯èƒ½å¢åŠ è¯¯æ£€ï¼‰"
            )
        
        
        btn_col1, btn_col2 = st.columns([1, 1]) 
        with btn_col1:
            
            detect_btn = st.button(
                "ğŸš€ å¼€å§‹æ£€æµ‹",
                disabled=not uploaded_file  # æ— æ–‡ä»¶æ—¶ç¦ç”¨æŒ‰é’®
            )
        with btn_col2:
            
            if st.button("ğŸ”„ é‡ç½®"):
                st.experimental_rerun()

        
        st.markdown("---")
        st.markdown("#### æ£€æµ‹ç»“æœ")
        
        
        result_placeholder = st.empty()
        info_placeholder = st.empty()

        # åˆå§‹çŠ¶æ€ï¼šæ˜¾ç¤º"ç­‰å¾…æ£€æµ‹"æç¤ºå›¾
        with result_placeholder:
            init_placeholder = np.ones((400, 600, 3), dtype=np.uint8) * 240  
            
            cv2.putText(
                init_placeholder, "",
                (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (100, 100, 100), 2, cv2.LINE_AA
            )
            st.image(init_placeholder, caption="ç­‰å¾…æ£€æµ‹", width=600)
        
       
        with info_placeholder:
            st.info("â„¹ï¸ æ”¯æŒæ ¼å¼ï¼šPNG/JPG/JPEG/BMPï¼Œå»ºè®®å›¾åƒä¸­èœœèœ‚æ¸…æ™°å¯è§")

        # æ£€æµ‹é€»è¾‘ï¼šç‚¹å‡»"å¼€å§‹æ£€æµ‹"ä¸”æœ‰ä¸Šä¼ æ–‡ä»¶æ—¶æ‰§è¡Œ
        if detect_btn and uploaded_file:
            try:
                 
                
                uploaded_bytes = uploaded_file.read()
                
                img_array = np.asarray(bytearray(uploaded_bytes), dtype=np.uint8)
               
                img_bgr = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                #------------------------------------------------------------------------------------------
                
                # æ£€æŸ¥å›¾åƒè§£ç æ˜¯å¦æˆåŠŸ
                if img_bgr is None:
                    raise ValueError("å›¾åƒè§£ç å¤±è´¥ï¼Œæ–‡ä»¶å¯èƒ½æŸåæˆ–æ ¼å¼ä¸æ”¯æŒ")
                
                # è½¬æ¢é¢œè‰²ç©ºé—´
                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

                # æ‰§è¡Œæ£€æµ‹ï¼ˆæ˜¾ç¤ºåŠ è½½ä¸­æç¤ºï¼‰
                with st.spinner("ğŸ” æ­£åœ¨æ£€æµ‹èœœèœ‚..."):
                    pred_boxes = predict_image(model, img_rgb, conf_threshold)

                # ç»˜åˆ¶é¢„æµ‹æ¡†ï¼ˆçº¢è‰²æ¡†ï¼š(255,0,0)ï¼‰
                img_with_pred = draw_boxes(img_rgb, pred_boxes, color=(255, 0, 0))

                # æ›´æ–°ç»“æœåŒºï¼šæ˜¾ç¤ºæ£€æµ‹åçš„å›¾åƒ
                with result_placeholder:
                    st.image(
                        img_with_pred,
                        caption=f"æ£€æµ‹ç»“æœï¼ˆç½®ä¿¡åº¦é˜ˆå€¼ï¼š{conf_threshold}ï¼‰",
                        width=600,
                        clamp=True  # é˜²æ­¢å›¾åƒæº¢å‡º
                    )
                
                # æ›´æ–°ä¿¡æ¯åŒºï¼šæ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡ç»“æœ
                with info_placeholder:
                    if pred_boxes:
                        max_conf = max([box[4] for box in pred_boxes])  # æœ€é«˜ç½®ä¿¡åº¦
                        st.success(f"""
                        ğŸ‰ æ£€æµ‹å®Œæˆï¼
                        - å…±è¯†åˆ«åˆ° **{len(pred_boxes)} åªèœœèœ‚**
                        - æœ€é«˜ç½®ä¿¡åº¦ï¼š{max_conf:.2f}
                        - ç½®ä¿¡åº¦é˜ˆå€¼ï¼š{conf_threshold}
                        """)
                    else:
                        st.warning(f"""
                        âš ï¸ æœªè¯†åˆ«åˆ°èœœèœ‚
                        - å»ºè®®ï¼šé™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå½“å‰ï¼š{conf_threshold}ï¼‰ï¼Œæˆ–ä¸Šä¼ èœœèœ‚æ›´æ¸…æ™°çš„å›¾åƒ
                        - æ£€æŸ¥ï¼šç¡®ä¿å›¾åƒä¸­èœœèœ‚å°ºå¯¸é€‚ä¸­ï¼ˆé¿å…è¿‡å°æˆ–è¿‡å¤§ï¼‰
                        """)

            # æ•è·æ£€æµ‹è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
            except Exception as e:
                # å¼‚å¸¸çŠ¶æ€ï¼šæ˜¾ç¤ºé”™è¯¯æç¤ºå›¾
                with result_placeholder:
                    error_placeholder = np.ones((400, 600, 3), dtype=np.uint8) * 240
                    cv2.putText(
                        error_placeholder, "æ£€æµ‹å‡ºé”™ï¼",
                        (180, 180), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3, cv2.LINE_AA
                    )
                    cv2.putText(
                        error_placeholder, str(e),
                        (50, 250), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2, cv2.LINE_AA
                    )
                    st.image(error_placeholder, caption="æ£€æµ‹é”™è¯¯", width=600)
                
                # å¼‚å¸¸ä¿¡æ¯æç¤º
                with info_placeholder:
                    st.error(f"âŒ æ£€æµ‹å¤±è´¥ï¼š{str(e)}")
if __name__ == "__main__":
    main()
