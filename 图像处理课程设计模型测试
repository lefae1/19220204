import os
import numpy as np
import paddle
import paddle.fluid as fluid
import paddle.nn as nn
from paddle.io import Dataset, DataLoader
import cv2
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from tqdm import tqdm
import math

# 配置参数
DATA_DIR = "/home/aistudio/bee_dataset"
TEST_DIR = os.path.join(DATA_DIR, "test")
TEST_ANNOTATION = os.path.join(TEST_DIR, "_annotations.coco.json")
IMG_SIZE = 640
NUM_CLASSES = 1
ANCHORS = [
    10, 13, 16, 30, 33, 23,    # 小目标锚框
    30, 61, 62, 45, 59, 119,   # 中目标锚框
    116, 90, 156, 198, 373, 326 # 大目标锚框
]
ANCHOR_MASKS = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]  # 锚框掩码
MODEL_PATH = "models/best_model.pdparams"  # 已保存的模型路径

# 定义必要的网络组件（需与训练时完全一致）
class ConvBNLayer(nn.Layer):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, groups=1, act=True, act_type="leaky_relu"):
        super(ConvBNLayer, self).__init__()
        self.conv = nn.Conv2D(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=(kernel_size - 1) // 2,
            groups=groups,
            bias_attr=True  
        )
        if act_type not in ["leaky_relu", "relu"]:
            raise ValueError(f"不支持的激活函数类型: {act_type}")
        self.act = self._get_activation(act_type) if act else nn.Identity()

    def _get_activation(self, act_type):
        if act_type == "leaky_relu":
            return nn.LeakyReLU(negative_slope=0.1)
        elif act_type == "relu":
            return nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        x = self.act(x)
        return x

class ResidualBlock(nn.Layer):
    def __init__(self, in_channels, out_channels, shortcut=True):
        super(ResidualBlock, self).__init__()
        self.conv1 = ConvBNLayer(in_channels, out_channels, 3, act=True, act_type="leaky_relu")
        self.conv2 = ConvBNLayer(out_channels, out_channels, 3, act=True, act_type="leaky_relu")
        self.shortcut = shortcut

    def forward(self, x):
        residual = x
        x = self.conv1(x)
        x = self.conv2(x)
        if self.shortcut:
            x += residual
        return x

class C3(nn.Layer):
    def __init__(self, in_channels, out_channels, num_blocks, shortcut=True):
        super(C3, self).__init__()
        mid_channels = out_channels // 2
        self.conv1 = ConvBNLayer(in_channels, mid_channels, 1, act=True, act_type="leaky_relu")
        self.conv2 = ConvBNLayer(in_channels, mid_channels, 1, act=True, act_type="leaky_relu")
        self.conv3 = ConvBNLayer(mid_channels * 2, out_channels, 1, act=True, act_type="leaky_relu")
        self.blocks = nn.Sequential(
            *[ResidualBlock(mid_channels, mid_channels, shortcut=shortcut) for _ in range(num_blocks)]
        )

    def forward(self, x):
        return self.conv3(paddle.concat([self.blocks(self.conv1(x)), self.conv2(x)], axis=1))

class SPPF(nn.Layer):
    def __init__(self, in_channels, out_channels, k=5):
        super(SPPF, self).__init__()
        self.pool = nn.MaxPool2D(kernel_size=k, stride=1, padding=k//2)
        self.conv2 = nn.Sequential(
            nn.Conv2D(in_channels * 3, out_channels, 1, bias_attr=True),
            nn.LeakyReLU(negative_slope=0.1)
        )

    def forward(self, x):
        y1 = self.pool(x)
        y2 = self.pool(y1)
        concat_feat = paddle.concat([x, y1, y2], axis=1)
        return self.conv2(concat_feat)

class YOLOv5(nn.Layer):
    def __init__(self, num_classes=1, anchors=ANCHORS):
        super(YOLOv5, self).__init__()
        self.num_classes = num_classes
        self.num_anchors = 3
        self.anchors = paddle.to_tensor(anchors, dtype=paddle.float32).reshape([-1, 2])
        
        # 主干网络
        self.backbone = nn.Sequential(
            ConvBNLayer(3, 32, 6, 2, act=True, act_type="leaky_relu"),
            ConvBNLayer(32, 64, 3, 2, act=True, act_type="leaky_relu"),
            C3(64, 64, 1),
            ConvBNLayer(64, 128, 3, 2, act=True, act_type="leaky_relu"),
            C3(128, 128, 3),
            ConvBNLayer(128, 256, 3, 2, act=True, act_type="leaky_relu"),
            C3(256, 256, 6),
            ConvBNLayer(256, 512, 3, 2, act=True, act_type="leaky_relu"),
            C3(512, 512, 3),
            SPPF(512, 512),
        )
        
        # 颈部网络
        self.neck = nn.Sequential(
            ConvBNLayer(512, 256, 1, act=True, act_type="leaky_relu"),
            nn.Upsample(scale_factor=2, mode='nearest'),
            C3(512, 256, 1, shortcut=False),
            ConvBNLayer(256, 128, 1, act=True, act_type="leaky_relu"),
            nn.Upsample(scale_factor=2, mode='nearest'),
            C3(256, 128, 1, shortcut=False),
            ConvBNLayer(128, 128, 3, 2, act=True, act_type="leaky_relu"),
            C3(384, 256, 1, shortcut=False),
            ConvBNLayer(256, 256, 3, 2, act=True, act_type="leaky_relu"),
            C3(768, 512, 1, shortcut=False),
        )
        
        # 检测头
        self.head_small = nn.Conv2D(128, self.num_anchors*(5 + num_classes), 1)
        self.head_medium = nn.Conv2D(256, self.num_anchors*(5 + num_classes), 1)
        self.head_large = nn.Conv2D(512, self.num_anchors*(5 + num_classes), 1)

    def forward(self, x):
        # 主干网络特征提取
        x = self.backbone[0](x)
        x = self.backbone[1](x)
        x = self.backbone[2](x)
        x = self.backbone[3](x)
        x = self.backbone[4](x)
        feat80 = x
        
        x = self.backbone[5](x)
        x = self.backbone[6](x)
        feat40 = x
        
        x = self.backbone[7](x)
        x = self.backbone[8](x)
        x = self.backbone[9](x)
        feat20 = x
        
        # 颈部网络特征融合
        x = self.neck[0](feat20)
        x = self.neck[1](x)
        x = paddle.concat([x, feat40], axis=1)
        neck_feat40 = self.neck[2](x)
        
        x = self.neck[3](neck_feat40)
        x = self.neck[4](x)
        x = paddle.concat([x, feat80], axis=1)
        out_80 = self.neck[5](x)
        
        x = self.neck[6](out_80)
        x = paddle.concat([x, neck_feat40], axis=1)
        out_40 = self.neck[7](x)
        
        x = self.neck[8](out_40)
        x = paddle.concat([x, feat20], axis=1)
        out_20 = self.neck[9](x)
        
        # 检测头输出
        out80 = self.head_small(out_80)
        out40 = self.head_medium(out_40)
        out20 = self.head_large(out_20)
        
        return [out80, out40, out20]

# 数据集类
class BeeDataset(Dataset):
    def __init__(self, data_dir, annotation_file, img_size=640):
        super(BeeDataset, self).__init__()
        self.data_dir = data_dir
        self.img_size = img_size
        from pycocotools.coco import COCO
        self.coco = COCO(annotation_file)
        self.image_ids = self.coco.getImgIds()
        self.categories = self.coco.loadCats(self.coco.getCatIds())

    def __len__(self):
        return len(self.image_ids)
    
    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        
        # 加载图像
        img_path = os.path.join(self.data_dir, img_info['file_name'])
        img = cv2.imread(img_path)
        if img is None:
            raise ValueError(f"无法读取图像: {img_path}")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # 调整图像大小
        h, w = img.shape[:2]
        scale = min(self.img_size / w, self.img_size / h)
        new_w, new_h = int(w * scale), int(h * scale)
        img = cv2.resize(img, (new_w, new_h))
        
        # 创建画布并粘贴图像
        canvas = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
        canvas[:new_h, :new_w] = img
        img = canvas
        
        # 归一化并转换格式
        img = img.astype('float32') / 255.0
        img = img.transpose((2, 0, 1))  # HWC to CHW
        
        # 获取标注信息
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)
        
        targets = []
        for ann in anns:
            bbox = ann['bbox']
            x, y, bbox_w, bbox_h = bbox
            
            # 调整边界框坐标
            x = x * scale + (self.img_size - new_w) // 2
            y = y * scale + (self.img_size - new_h) // 2
            bbox_w *= scale
            bbox_h *= scale
            
            # 转换为YOLO格式并归一化
            center_x = (x + bbox_w / 2) / self.img_size
            center_y = (y + bbox_h / 2) / self.img_size
            norm_w = bbox_w / self.img_size
            norm_h = bbox_h / self.img_size
            class_id = ann['category_id'] - 1  # 类别ID从0开始
            
            targets.append([class_id, center_x, center_y, norm_w, norm_h])
        
        # 转换为paddle tensor
        img = paddle.to_tensor(img)
        targets = paddle.to_tensor(targets) if targets else paddle.zeros((0, 5))
        
        return img, targets, img_info['file_name']  # 额外返回文件名用于可视化

# 自定义collate函数
def collate_fn(batch):
    images = []
    targets_list = []
    filenames = []
    
    for img, tgt, filename in batch:
        images.append(img)
        filenames.append(filename)
        
        if tgt.shape[0] == 0:
            continue
            
        batch_indices = paddle.full((tgt.shape[0], 1), len(images)-1, dtype=paddle.float32)
        class_ids = tgt[:, 0:1].cast(paddle.float32)
        bboxes = tgt[:, 1:5].cast(paddle.float32)
        tgt = paddle.concat([batch_indices, class_ids, bboxes], axis=1)
        targets_list.append(tgt)
    
    images = paddle.stack(images)
    
    if len(targets_list) == 0:
        targets = paddle.zeros((0, 6), dtype=paddle.float32)
    else:
        targets = paddle.concat(targets_list, axis=0)
    
    return images, targets, filenames

# 非极大值抑制
def nms(boxes, scores, iou_threshold=0.5):
    if len(boxes) == 0:
        return []
    
    sorted_indices = paddle.argsort(scores, descending=True)
    boxes = boxes[sorted_indices]
    scores = scores[sorted_indices]
    
    keep = []
    while len(boxes) > 0:
        keep.append(sorted_indices[0].numpy()[0])
        ious = box_iou(boxes[0:1], boxes[1:])
        
        mask = ious < iou_threshold
        boxes = boxes[1:][mask[0]]
        sorted_indices = sorted_indices[1:][mask[0]]
    
    return keep

def box_iou(box1, box2):
    box1_x1 = box1[:, 0]
    box1_y1 = box1[:, 1]
    box1_x2 = box1[:, 2]
    box1_y2 = box1[:, 3]
    box2_x1 = box2[:, 0]
    box2_y1 = box2[:, 1]
    box2_x2 = box2[:, 2]
    box2_y2 = box2[:, 3]

    inter_x1 = fluid.layers.maximum(box1_x1, box2_x1)
    inter_y1 = fluid.layers.maximum(box1_y1, box2_y1)
    inter_x2 = fluid.layers.minimum(box1_x2, box2_x2)
    inter_y2 = fluid.layers.minimum(box1_y2, box2_y2)
    
    inter_area = fluid.layers.maximum(0, inter_x2 - inter_x1) * fluid.layers.maximum(0, inter_y2 - inter_y1)
    
    area1 = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)
    area2 = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)
    union_area = area1 + area2 - inter_area + 1e-9
    
    return inter_area / union_area

# 评估函数
def evaluate_model(model, test_loader, img_size=640, conf_threshold=0.5, iou_threshold=0.5):
    model.eval()
    total_tp = 0
    total_fp = 0
    total_fn = 0
    all_preds = []
    all_targets = []
    
    with fluid.dygraph.no_grad():
        for data, targets, _ in tqdm(test_loader, desc='Evaluating'):
            batch_size = data.shape[0]
            outputs = model(data)  # outputs包含3个尺度的特征图
            
            # 收集所有目标
            for i in range(batch_size):
                mask = targets[:, 0] == i
                if mask.any():
                    all_targets.append(targets[mask][:, 1:6])
                else:
                    all_targets.append(paddle.zeros((0, 5)))
            
            # 处理预测结果
            for scale_idx in range(3):  
                out = outputs[scale_idx]
                batch_size, _, grid_h, grid_w = out.shape
                stride = img_size / grid_h
                
                # 根据尺度索引获取对应的锚框掩码
                mask = ANCHOR_MASKS[scale_idx]
                
                # 提取锚框
                anchor_indices = []
                for idx in mask:
                    anchor_indices.extend([idx*2, idx*2 + 1])
                anchors = paddle.to_tensor(ANCHORS, dtype=paddle.float32)[anchor_indices]
                anchors = anchors.reshape([-1, 2])
                
                # 重塑输出
                out = out.reshape([batch_size, 3, 5 + NUM_CLASSES, grid_h, grid_w])
                out = out.transpose([0, 1, 3, 4, 2])
                
                # 解析预测结果
                xy = fluid.layers.sigmoid(out[..., :2])
                wh = fluid.layers.exp(out[..., 2:4]) * anchors.reshape([1, 3, 1, 1, 2])
                conf = fluid.layers.sigmoid(out[..., 4:5])
                cls = fluid.layers.sigmoid(out[..., 5:])
                
                # 生成网格坐标
                grid_x = paddle.arange(grid_w).reshape([1, 1, 1, grid_w, 1]).tile([batch_size, 3, grid_h, 1, 1])
                grid_y = paddle.arange(grid_h).reshape([1, 1, grid_h, 1, 1]).tile([batch_size, 3, 1, grid_w, 1])
                
                # 计算绝对坐标
                x = (xy[..., 0:1] + grid_x) * stride
                y = (xy[..., 1:2] + grid_y) * stride
                w = wh[..., 0:1]
                h = wh[..., 1:2]
                
                # 转换为x1, y1, x2, y2格式
                x1 = x - w / 2
                y1 = y - h / 2
                x2 = x + w / 2
                y2 = y + h / 2
                
                # 拼接结果
                pred = paddle.concat([x1, y1, x2, y2, conf, cls], axis=-1)
                pred = pred.reshape([batch_size, -1, 5 + NUM_CLASSES])
                
                # 合并多尺度预测
                if scale_idx == 0:
                    preds = [pred]
                else:
                    preds.append(pred)
            
            # 合并三个尺度的预测结果
            preds = paddle.concat(preds, axis=1)
            
            # 应用置信度阈值和NMS
            for i in range(batch_size):
                pred = preds[i]
                mask = pred[:, 4] > conf_threshold
                if mask.any():
                    pred = pred[mask]
                    keep = nms(pred[:, :4], pred[:, 4], iou_threshold)
                    pred = pred[keep]
                    all_preds.append(pred)
                else:
                    all_preds.append(paddle.zeros((0, 5 + NUM_CLASSES)))
    
    # 计算评估指标
    precision, recall, f1 = calculate_metrics(all_preds, all_targets, iou_threshold=0.5)
    print(f"评估结果 - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}")
    return precision, recall, f1

def calculate_metrics(preds, targets, iou_threshold=0.5):
    total_tp = 0
    total_fp = 0
    total_fn = 0
    
    for pred, target in zip(preds, targets):
        if len(pred) == 0 and len(target) == 0:
            continue
        elif len(pred) == 0:
            total_fn += len(target)
            continue
        elif len(target) == 0:
            total_fp += len(pred)
            continue
        
        ious = []
        for p in pred:
            iou = box_iou(p[None, :4], target[:, 1:5])
            ious.append(iou.numpy())
        
        ious = np.array(ious)
        detected = np.zeros(len(target))
        tp = 0
        
        pred_indices = np.argsort(pred[:, 4].numpy())[::-1]
        
        for i in pred_indices:
            max_iou = np.max(ious[i])
            max_idx = np.argmax(ious[i])
            
            if max_iou > iou_threshold and not detected[max_idx]:
                tp += 1
                detected[max_idx] = 1
        
        fp = len(pred) - tp
        fn = len(target) - np.sum(detected)
        
        total_tp += tp
        total_fp += fp
        total_fn += fn
    
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return precision, recall, f1

# 可视化预测结果
def visualize_predictions(model, dataset, num_samples=5, conf_threshold=0.5):
    model.eval()
    fig, axes = plt.subplots(num_samples, 1, figsize=(10, num_samples*6))
    
    with fluid.dygraph.no_grad():
        for i in range(num_samples):
            img, targets, filename = dataset[i]
            img_np = img.numpy().transpose((1, 2, 0))
            
            # 获取预测结果
            preds = model(img[None, ...])  # 三个尺度的特征图
            
            # 处理预测结果
            all_boxes = []
            all_scores = []
            
            # 直接遍历三个尺度，通过索引获取，避免使用index方法
            for scale_idx in range(3):
                out = preds[scale_idx]
                batch_size, _, grid_h, grid_w = out.shape
                stride = IMG_SIZE / grid_h
                mask = ANCHOR_MASKS[scale_idx]  # 尺度索引直接对应掩码
                
                # 提取锚框
                anchor_indices = []
                for idx in mask:
                    anchor_indices.extend([idx*2, idx*2 + 1])
                anchors = paddle.to_tensor(ANCHORS, dtype=paddle.float32)[anchor_indices].reshape([-1, 2])
                
                # 重塑输出
                out = out.reshape([batch_size, 3, 5 + NUM_CLASSES, grid_h, grid_w])
                out = out.transpose([0, 1, 3, 4, 2])
                
                # 解析预测结果
                xy = fluid.layers.sigmoid(out[..., :2])
                wh = fluid.layers.exp(out[..., 2:4]) * anchors.reshape([1, 3, 1, 1, 2])
                conf = fluid.layers.sigmoid(out[..., 4])
                
                # 生成网格坐标
                grid_x = paddle.arange(grid_w).reshape([1, 1, 1, grid_w, 1]).tile([batch_size, 3, grid_h, 1, 1])
                grid_y = paddle.arange(grid_h).reshape([1, 1, grid_h, 1, 1]).tile([batch_size, 3, 1, grid_w, 1])
                
                # 计算绝对坐标
                x = (xy[..., 0] + grid_x[..., 0]) * stride
                y = (xy[..., 1] + grid_y[..., 0]) * stride
                w = wh[..., 0]
                h = wh[..., 1]
                
                # 转换为x1, y1, x2, y2格式
                x1 = x - w / 2
                y1 = y - h / 2
                x2 = x + w / 2
                y2 = y + h / 2
                
                # 过滤置信度阈值并收集结果
                mask_conf = conf > conf_threshold
                if mask_conf.any():
                    boxes = paddle.stack([x1[mask_conf], y1[mask_conf], x2[mask_conf], y2[mask_conf]], axis=1)
                    scores = conf[mask_conf]
                    all_boxes.append(boxes.numpy())
                    all_scores.append(scores.numpy())
            
            # 合并多尺度结果并应用非极大值抑制
            if all_boxes:
                boxes = paddle.to_tensor(np.concatenate(all_boxes, axis=0))
                scores = paddle.to_tensor(np.concatenate(all_scores, axis=0))
                keep = nms(boxes, scores)
                boxes = boxes[keep].numpy()
            else:
                boxes = []
            
            # 绘制图像和边界框
            axes[i].imshow(img_np)
            axes[i].set_title(f'预测示例 {i+1}: {filename}')
            axes[i].axis('off')
            
            # 绘制预测框和真实框
            for box in boxes:
                x1, y1, x2, y2 = box
                rect = Rectangle((x1, y1), x2-x1, y2-y1, 
                                fill=False, color='red', linewidth=2)
                axes[i].add_patch(rect)
            
            for tgt in targets.numpy():
                cls_id, cx, cy, w, h = tgt
                x1 = (cx - w/2) * IMG_SIZE
                y1 = (cy - h/2) * IMG_SIZE
                x2 = (cx + w/2) * IMG_SIZE
                y2 = (cy + h/2) * IMG_SIZE
                rect = Rectangle((x1, y1), x2-x1, y2-y1, 
                                fill=False, color='green', linewidth=2, linestyle='--')
                axes[i].add_patch(rect)
    
    plt.tight_layout()
    plt.savefig('prediction_examples.png')
    plt.show()

# 主测试函数
def main():
    # 创建测试数据集和数据加载器
    test_dataset = BeeDataset(TEST_DIR, TEST_ANNOTATION, IMG_SIZE)
    test_loader = DataLoader(
        test_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=0,  
        collate_fn=collate_fn
    )
    print(f"测试集大小: {len(test_dataset)}")
    
    # 创建模型并加载权重
    print(f"加载模型: {MODEL_PATH}")
    model = YOLOv5(num_classes=NUM_CLASSES)
    # 加载模型权重
    if os.path.exists(MODEL_PATH):
        
        if 'best_model' in MODEL_PATH:
            checkpoint = paddle.load(MODEL_PATH)
            model.set_state_dict(checkpoint['model_state_dict'])
        else:
           
            model.set_state_dict(paddle.load(MODEL_PATH))
        print("模型加载成功")
    else:
        raise FileNotFoundError(f"模型文件不存在: {MODEL_PATH}")
    
    # 评估模型
    print("开始评估模型...")
    precision, recall, f1 = evaluate_model(model, test_loader)
    
    # 可视化预测结果
    print("可视化预测结果...")
    visualize_predictions(model, test_dataset, num_samples=5)

if __name__ == "__main__":
    main()
